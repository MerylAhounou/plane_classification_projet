{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Nom | Prénom|\n",
    "|---|---|\n",
    "| AHOUNOU | Méryl |\n",
    "| KEVORKIAN | Amandine |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pathlib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPool2D, Dense, Flatten, Dropout\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "from yaml import load, Loader\n",
    "from PIL import Image\n",
    "from skimage.transform import resize\n",
    "from skimage.io import imread"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yaml file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml_file = open(\"app.yaml\", 'r')\n",
    "yaml_content = load(yaml_file, Loader=Loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = pathlib.Path(yaml_content[\"DATA_DIR\"])\n",
    "MODELS_DIR = yaml_content[\"MODELS_DIR\"]\n",
    "WEIGHTS_DIR = pathlib.Path(yaml_content[\"WEIGHTS_DIR\"])\n",
    "\n",
    "TARGET_NAME = yaml_content[\"TARGET_NAME\"]\n",
    "TARGET_NAME_TXT = f'images_{TARGET_NAME}_train.txt'\n",
    "\n",
    "IMAGE_WIDTH = yaml_content[\"IMAGE_WIDTH\"]\n",
    "IMAGE_HEIGHT = yaml_content[\"IMAGE_HEIGHT\"]\n",
    "IMAGE_DEPTH = yaml_content[\"IMAGE_DEPTH\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fonctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_image_database(path, target):\n",
    "    \"\"\"Build a pandas dataframe with target class and access path to images.\n",
    "    \n",
    "    Parameters\n",
    "    - - - - - -\n",
    "    path (Path): path patern to read csv file containing images information.\n",
    "    target (str): name of the target column.\n",
    "    \n",
    "    Returns\n",
    "    - - - - -\n",
    "    A pandas dataframe, including target class and path to image.\n",
    "    \"\"\"\n",
    "    _df = pd.read_csv(path, sep='\\t', \n",
    "            names=['all'],\n",
    "            dtype={'all': str} # ids are not int but string\n",
    "            )\n",
    "\n",
    "    # la fonction split() découpe sur une chaîne de charatères\n",
    "    _df['image_id'] = _df['all'].apply(lambda x: x.split(' ')[0])\n",
    "\n",
    "    # la fonction '<car>'.join(liste) concatène les éléments de liste en utilisant le séparateur <car>\n",
    "    _df[target] = _df['all'].apply(lambda x: ' '.join(x.split(' ')[1:]))\n",
    "\n",
    "    # la colonne path contient le chemin d'accès à l'image\n",
    "    _df['path'] = _df['image_id'].apply(lambda x:  pathlib.Path('../data/dataset/data/images') / (x + '.jpg'))\n",
    "\n",
    "    return _df.drop(columns=['all'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_x_y(df: pd.DataFrame, target: str, images: str, type_model, stratify=None):\n",
    "    \"\"\"Build x tensor and y tensor for model fitting.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df (pd.DataFrame): dataframe containing images and target\n",
    "    target (str): name of target column\n",
    "    images (str): name of images column\n",
    "    type_model (int): type of model\n",
    "                    0 Keras model\n",
    "                    1 SVM\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    x (np.array/pd.DataFrame): tensor of x values / DataFrame of multiple columns whose each column represent pixel position\n",
    "    y (np.array): tensor / array of y values \n",
    "    \"\"\"\n",
    "    if type_model ==0:\n",
    "      x = np.array(df[images].to_list())\n",
    "      y = to_categorical(df[target].astype('category').cat.codes)\n",
    "    elif type_model ==1:\n",
    "      x = pd.DataFrame(np.array([img for img in df[images]]))\n",
    "      y = df[target].astype('category').cat.codes\n",
    "    else:\n",
    "      print('The type of model is not correct. Please insert a number between 0-2.\\nPS:\\n0 Keras model\\n1 SVM')  \n",
    "\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_classification_model(df, target: str, images: str):\n",
    "    \"\"\"Build a TF model using information from target and images columns in dataframe\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df (pd.DataFrame): dataframe with target and images columns\n",
    "    target(str): column name for target variable\n",
    "    images(str): column name for images\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    TF model built & compiled\n",
    "    \"\"\"\n",
    "    nb_classes = df[target].nunique() # Compute number of classes for output layer\n",
    "    size = df[images].iloc[0].shape # Compute images size for input layer\n",
    "    \n",
    "    #Building the model\n",
    "    model = Sequential()\n",
    "    model.add(layers.RandomFlip(\"horizontal\"))\n",
    "    model.add(layers.RandomRotation(0.1))\n",
    "    model.add(layers.Rescaling(scale=1 / 127.5, offset=-1))\n",
    "    model.add(Conv2D(filters=32, kernel_size=(5,5), activation='relu', input_shape=(size)))\n",
    "    model.add(Conv2D(filters=32, kernel_size=(5,5), activation='relu'))\n",
    "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(rate=0.25))\n",
    "    model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(rate=0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(rate=0.25))\n",
    "    model.add(Dense(nb_classes, activation='softmax')) # Couche de sortie à nb_classes\n",
    "\n",
    "    #Compilation of the model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_images(images, model, type_model=0, classes_names=None):\n",
    "      \"\"\"Classify images through a TF model.\n",
    "      Parameters\n",
    "      ----------\n",
    "      images (np.array): set of images to classify\n",
    "      model (model): TF/ Keras model / SVM\n",
    "      type_model (int): type of model\n",
    "                        0 Keras model\n",
    "                        1 SVM\n",
    "      classes_names: dictionnary with classes names\n",
    "      Returns\n",
    "      -------\n",
    "      predicted classes\n",
    "      \"\"\"\n",
    "      if type_model ==0:\n",
    "            results = model.predict(images) # predict for images\n",
    "            classes = np.argmax(results, axis=1) # np.argmax returns the index of the max value per row\n",
    "            if classes_names is not None:\n",
    "                  classes = np.array(classes_names[classes])\n",
    "      elif type_model==1:\n",
    "            results = model.predict(images) # predict for images\n",
    "            if classes_names is not None:\n",
    "                  classes = np.array(classes_names[results])\n",
    "      else:\n",
    "            print('The type of model is not correct. Please insert a number between 0-2.\\nPS:\\n0 Keras model\\n1 SVM')   \n",
    "      return classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_metrics(df_test, target, y_pred, y_test, images, model, type_model, ret):\n",
    "    \"\"\"Give some metrics for model evaluation\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df_test (pd.series): DataFrame which contains the test set\n",
    "    target (str): the target variable\n",
    "    y_pred (): the model prediction\n",
    "    y_test (): \n",
    "    images (np.array): set of images to classify\n",
    "    model (model): TF/ Keras model / SVM\n",
    "    type_model (int): type of model\n",
    "                    0 Keras model\n",
    "                    1 SVM\n",
    "    ret (int): to print or not\n",
    "    Returns:\n",
    "    accuracy, precision and recall\n",
    "    --------\n",
    "    \"\"\"\n",
    "    if type_model ==0:\n",
    "      y_pred = np.argmax(y_pred, axis=1)\n",
    "      y_test = df_test[target].astype('category').cat.codes\n",
    "      if ret ==1:\n",
    "        print(f'accuracy: {accuracy_score(y_pred,y_test)*100:.2f}%')\n",
    "        print(f'precision: {precision_score(y_pred,y_test, average=\"macro\")*100:.2f}%')\n",
    "        print(f'recall: {recall_score(y_pred,y_test, average=\"macro\")*100:.2f}%')\n",
    "        print(classification_report(y_test, y_pred))\n",
    "        fig, ax = plt.subplots(figsize=(15,10))\n",
    "        sns.heatmap(pd.crosstab(df_test[target], \n",
    "                                classify_images(images, model, type_model, df_test[target].astype('category').cat.categories), \n",
    "                                normalize='index'),\n",
    "                    cmap='vlag',\n",
    "                    ax=ax)\n",
    "      elif ret ==0:\n",
    "        return f'{accuracy_score(y_pred, y_test)*100:.2f}%', f'{precision_score(y_pred, y_test, average=\"macro\")*100:.2f}%', f'{recall_score(y_pred,y_test, average=\"macro\")*100:.2f}%'  \n",
    "    elif type_model ==1:\n",
    "      if ret ==1:\n",
    "        print(f'accuracy: {accuracy_score(y_pred,y_test)*100:.2f}%')\n",
    "        print(f'precision: {precision_score(y_pred,y_test, average=\"macro\")*100:.2f}%')\n",
    "        print(f'recall: {recall_score(y_pred,y_test, average=\"macro\")*100:.2f}%')\n",
    "        print(classification_report(y_test, y_pred))\n",
    "        fig, ax = plt.subplots(figsize=(15,10))\n",
    "        sns.heatmap(pd.crosstab(df_test[target], \n",
    "                                classify_images(images, model, type_model, df_test[target].astype('category').cat.categories), \n",
    "                                normalize='index'),\n",
    "                    cmap='vlag',\n",
    "                    ax=ax)\n",
    "      elif ret ==0:\n",
    "        return f'{accuracy_score(y_pred, y_test)*100:.2f}%', f'{precision_score(y_pred, y_test, average=\"macro\")*100:.2f}%', f'{recall_score(y_pred,y_test, average=\"macro\")*100:.2f}%'\n",
    "    else:\n",
    "      print('The type of model is not correct. Please insert a number between 0-2.\\nPS:\\n0 Keras model\\n1 SVM')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_target_class_liss(df, path, target):\n",
    "    \"\"\"Generate a txt file that contains the differents target class\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    df (pd.DataFrame): df which contains the variable whose classes we want to know\n",
    "    path (Path): path to save the txt file\n",
    "    target (str): the target variable\n",
    "    Returns:\n",
    "    -------\n",
    "    \"\"\"\n",
    "    path = path + '/' + target + '_classes.txt'\n",
    "    with open(path, 'w') as f:\n",
    "        for name in [df[target].astype('category').cat.categories][0]:\n",
    "            f.write('%s\\n' %name)\n",
    "        f.close()\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_resize_image(path, type_model, height, width):\n",
    "    \"\"\"Load an image and resize it to the target size\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    path (Path): access path to the image file\n",
    "    height (int): resize image to this height\n",
    "    width (int): resize to this width\n",
    "    type_model (int): type of model\n",
    "                    0 Keras model\n",
    "                    1 SVM\n",
    "    Returns\n",
    "    --------\n",
    "    np.array containing resized image / flaten image\n",
    "    \"\"\"\n",
    "    if type_model ==0:\n",
    "      return np.array(Image.open(path).resize((width, height)))\n",
    "    elif type_model ==1:\n",
    "      return (resize(imread(path)/255,(width,height))).flatten()\n",
    "    else:\n",
    "      print('The type of model is not correct. Please insert a number between 0-2.\\nPS:\\n0 Keras model\\n1 SVM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    \"\"\"Plot somes fig to see the model progress\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    history (keras.callbacks.History) : the model training history\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    \"\"\"\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    epochs = range(len(acc))\n",
    "    plt.plot(epochs, acc, 'b', label='Training acc')\n",
    "    plt.plot(epochs, val_acc, 'r', label='Validation acc')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.legend()\n",
    "    plt.figure()\n",
    "    plt.plot(epochs, loss, 'b', label='Training loss')\n",
    "    plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, type_model, basename, target):\n",
    "      \"\"\"Save tf/Keras model\n",
    "\n",
    "      Model file is named model + timestamp.\n",
    "\n",
    "      Parameters\n",
    "      ----------\n",
    "      model (model): TF/ Keras model / SVM\n",
    "      type_model (int): type of model\n",
    "                        0 CNN neural network\n",
    "                        1 SVM\n",
    "                        2 Xception\n",
    "                        3 VGG19\n",
    "      basename: location to save model file\n",
    "      target (int): the target variable name. \n",
    "      \"\"\"\n",
    "      if type_model == 0:\n",
    "            model.save(f'{basename}/{target}_cnn.h5')\n",
    "      elif type_model == 1:\n",
    "            pickle.dump(model,open(f'{basename}/{target}_svm.p','wb'))\n",
    "      elif type_model == 2:\n",
    "            model.save(f'{basename}/{target}_xception.h5')\n",
    "      elif type_model == 3:\n",
    "            model.save(f'{basename}/{target}_vgg19.h5')\n",
    "      elif type_model == 4:\n",
    "            pickle.dump(model,open(f'{basename}/{target}_pca.p','wb'))\n",
    "      else:\n",
    "            print('The type of model is not correct. Please insert a number between 0-2.\\nPS:\\n0 CNN neural network\\n1 SVM\\n2 Xception\\n3 VGG19')\n",
    "      return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(df, row, target ):\n",
    "    \"\"\"Show an image from an image database, with the associated class.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df (pd.DataFrame): images definition dataframe\n",
    "    row (int): row index in df of image to be displayed\n",
    "    target (str): name of the target column\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "    assert target in df.columns, 'Missing target column in dataframe'\n",
    "    assert 'path' in df.columns, 'Missing image path in dataframe'\n",
    "    print(df.iloc[row,][target])\n",
    "    plt.imshow(plt.imread(df.iloc[row,]['path']))\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading files (train & test, images and class information)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = build_image_database(DATA_DIR / TARGET_NAME_TXT, TARGET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert df['image_id'].isna().sum() ==0, 'Valeur manquante dans image'\n",
    "assert df[TARGET_NAME].isna().sum() ==0, 'Valeur manquante dans image'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[TARGET_NAME].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image(df, 42, TARGET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['shape_image'] = df['path'].apply(lambda p: plt.imread(p).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution du nombre de lignes\n",
    "df.shape_image.apply(lambda x: x[0]).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution du nombre de colonnes\n",
    "df.shape_image.apply(lambda x: x[1]).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['resized_image'] = df.apply(lambda r: load_resize_image(r['path'], 0, IMAGE_HEIGHT, IMAGE_WIDTH), axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[TARGET_NAME].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate txt of differents classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_target_class_liss(df, MODELS_DIR, TARGET_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train & test dataset\n",
    "train_df = build_image_database(DATA_DIR / TARGET_NAME_TXT, TARGET_NAME)\n",
    "test_df = build_image_database(DATA_DIR / TARGET_NAME_TXT, TARGET_NAME)\n",
    "\n",
    "# Load & resize images\n",
    "train_df['resized_image'] = train_df.apply(lambda r: load_resize_image(r['path'], 0,\n",
    "                                                                       IMAGE_HEIGHT, IMAGE_WIDTH),\n",
    "                                           axis=1)\n",
    "test_df['resized_image'] = test_df.apply(lambda r: load_resize_image(r['path'], 0,\n",
    "                                                                       IMAGE_HEIGHT, IMAGE_WIDTH),\n",
    "                                           axis=1)\n",
    "\n",
    "# Build tensors for training & testing\n",
    "X_train, y_train = build_x_y(train_df, TARGET_NAME, 'resized_image', 0)\n",
    "X_test, y_test = build_x_y(test_df, TARGET_NAME, 'resized_image', 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_classification_model(train_df, TARGET_NAME, 'resized_image')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard\n",
    "\n",
    "!del -rf ./logs\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Add checkpoint function to save best model\n",
    "checkpoint = ModelCheckpoint(f'{WEIGHTS_DIR}/best_model_cnn.hdf5', \n",
    "                             monitor = 'val_accuracy', \n",
    "                             verbose = 0,\n",
    "                             save_best_only=True,\n",
    "                             save_weights_only=True,\n",
    "                             mode = 'max')\n",
    "\n",
    "# Add LR scheduler \n",
    "lr_scheduler = ReduceLROnPlateau(monitor='val_loss',\n",
    "                              factor=0.2, #let's reduce LR 5 times\n",
    "                              patience=3, # if no improvement after 3 epoch - reduce LR\n",
    "                              min_lr=0.0001,\n",
    "                              verbose=1,\n",
    "                              mode='auto')\n",
    "\n",
    "# Add early stop\n",
    "earlystop = EarlyStopping(monitor = 'val_accuracy',\n",
    "                          patience = 10,\n",
    "                          restore_best_weights = True)\n",
    "\n",
    "callbacks_list = [earlystop, lr_scheduler, tensorboard_callback, checkpoint]\n",
    "\n",
    "epochs = 60\n",
    "history = model.fit(X_train, y_train, batch_size=16, epochs=epochs, \n",
    "                    validation_data=(X_test, y_test),\n",
    "                    callbacks=[callbacks_list]\n",
    "                    # callbacks=[tensorboard_callback]\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(f'{WEIGHTS_DIR}/best_model_cnn.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(model, 0, MODELS_DIR, TARGET_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load & resize images\n",
    "train_df['resized_image_svm'] = train_df.apply(lambda r: load_resize_image(r['path'], 1,\n",
    "                                                                       IMAGE_HEIGHT, IMAGE_WIDTH),\n",
    "                                           axis=1)\n",
    "test_df['resized_image_svm'] = test_df.apply(lambda r: load_resize_image(r['path'], 1,\n",
    "                                                                       IMAGE_HEIGHT, IMAGE_WIDTH),\n",
    "                                           axis=1)\n",
    "\n",
    "# Build tensors for training & testing\n",
    "X_train_svm, y_train_svm = build_x_y(train_df, 'manufacturer', 'resized_image_svm', 1)\n",
    "X_test_svm, y_test_svm = build_x_y(test_df, 'manufacturer', 'resized_image_svm', 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.fit(X_train_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_svm = pca.transform(X_train_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_svm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_svm = pca.transform(X_test_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_svm.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(pca, 4, MODELS_DIR, TARGET_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = svm.SVC(kernel = 'linear', probability=True)\n",
    "model_svm = svc.fit(X_train_svm, y_train_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turning hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_grid = {'C': [0.1, 1, 10, 100], \n",
    "#               'gamma': [1, 0.1, 0.01, 0.001],\n",
    "#               'kernel': ['rbf', 'poly', 'sigmoid', 'linear']\n",
    "#               }\n",
    "\n",
    "# model_svm_grid=GridSearchCV(svc,param_grid)\n",
    "# model_svm_grid.fit(X_train_svm,y_train_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_svm_grid.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(model_svm, 1, MODELS_DIR, TARGET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_model(model_svm_grid, 1, MODELS_DIR, TARGET_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfert learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Xception"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classes = train_df[TARGET_NAME].nunique()\n",
    "base_model = keras.applications.Xception(\n",
    "    weights=\"imagenet\",  # Load weights pre-trained on ImageNet.\n",
    "    input_shape=(IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_DEPTH),\n",
    "    include_top=False,\n",
    "    classifier_activation=\"softmax\"\n",
    ")  # Do not include the ImageNet classifier at the top.\n",
    "\n",
    "# Freeze the base_model\n",
    "base_model.trainable = False\n",
    "\n",
    "data_augmentation = keras.Sequential(\n",
    "    [layers.RandomFlip(\"horizontal\"), layers.RandomRotation(0.1),]\n",
    ")\n",
    "\n",
    "\n",
    "# Create new model on top\n",
    "inputs = keras.Input(shape=(IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_DEPTH))\n",
    "x = data_augmentation(inputs)  # Apply random data augmentation\n",
    "\n",
    "# Pre-trained Xception weights requires that input be scaled\n",
    "# from (0, 255) to a range of (-1., +1.), the rescaling layer\n",
    "# outputs: `(inputs * scale) + offset`\n",
    "scale_layer = keras.layers.Rescaling(scale=1 / 127.5, offset=-1)\n",
    "x = scale_layer(x)\n",
    "\n",
    "# The base model contains batchnorm layers. We want to keep them in inference mode\n",
    "# when we unfreeze the base model for fine-tuning, so we make sure that the\n",
    "# base_model is running in inference mode here.\n",
    "x = base_model(x, training=False)\n",
    "x = keras.layers.GlobalAveragePooling2D()(x)\n",
    "x = keras.layers.Dropout(0.2)(x)  # Regularize with dropout\n",
    "outputs = keras.layers.Dense(nb_classes)(x)\n",
    "model_xception = keras.Model(inputs, outputs)\n",
    "\n",
    "model_xception.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model_Xception.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "# # Add checkpoint function to save best model\n",
    "checkpoint = ModelCheckpoint(f'{WEIGHTS_DIR}/best_model_xception.hdf5', \n",
    "                             monitor = 'val_accuracy', \n",
    "                             verbose = 0,\n",
    "                             save_best_only=True,\n",
    "                             save_weights_only=True,\n",
    "                             mode = 'max')\n",
    "\n",
    "# Add LR scheduler \n",
    "lr_scheduler = ReduceLROnPlateau(monitor='val_loss',\n",
    "                              factor=0.2, #let's reduce LR 5 times\n",
    "                              patience=3, # if no improvement after 3 epoch - reduce LR\n",
    "                              min_lr=0.0001,\n",
    "                              verbose=1,\n",
    "                              mode='auto')\n",
    "\n",
    "# Add early stop\n",
    "earlystop = EarlyStopping(monitor = 'val_accuracy',\n",
    "                          patience = 10,\n",
    "                          restore_best_weights = True)\n",
    "\n",
    "callbacks_list = [earlystop, lr_scheduler, checkpoint]\n",
    "\n",
    "epochs = 40\n",
    "history = model_Xception.fit(X_train, y_train, batch_size=32, epochs=epochs, \n",
    "                    validation_data=(X_test, y_test),\n",
    "                   callbacks = callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard\n",
    "\n",
    "!del -rf ./logs_x\n",
    "log_dir = \"logs_x/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unfreeze the base_model. Note that it keeps running in inference mode\n",
    "# since we passed `training=False` when calling it. This means that\n",
    "# the batchnorm layers will not update their batch statistics.\n",
    "# This prevents the batchnorm layers from undoing all the training\n",
    "# we've done so far.\n",
    "%%time\n",
    "base_model.trainable = True\n",
    "model_Xception.summary()\n",
    "\n",
    "model_Xception.compile(\n",
    "    optimizer=keras.optimizers.Adam(1e-5),  \n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "\n",
    "# # Add checkpoint function to save best model\n",
    "checkpoint = ModelCheckpoint(f'{WEIGHTS_DIR}/best_model_xception.hdf5', \n",
    "                             monitor = 'val_accuracy', \n",
    "                             verbose = 0,\n",
    "                             save_best_only=True,\n",
    "                             save_weights_only=True,\n",
    "                             mode = 'max')\n",
    "\n",
    "# Add LR scheduler \n",
    "lr_scheduler = ReduceLROnPlateau(monitor='val_loss',\n",
    "                              factor=0.1, \n",
    "                              patience=3, # if no improvement after 3 epoch - reduce LR\n",
    "                              min_lr=0.000001, \n",
    "                              verbose=1,\n",
    "                              mode='auto')\n",
    "\n",
    "# Add early stop\n",
    "earlystop = EarlyStopping(monitor = 'val_accuracy',\n",
    "                          patience = 10,\n",
    "                          restore_best_weights = True)\n",
    "\n",
    "callbacks_list = [earlystop, lr_scheduler, tensorboard_callback, checkpoint]\n",
    "\n",
    "\n",
    "epochs = 50\n",
    "history = model_Xception.fit(X_train, y_train, batch_size=64, epochs=epochs, \n",
    "                    validation_data=(X_test, y_test),\n",
    "                   callbacks = callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs_x/fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_xception.load_weights(f'{WEIGHTS_DIR}/best_model_xception.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(model_xception, 2, MODELS_DIR, TARGET_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classes = train_df[TARGET_NAME].nunique()\n",
    "base_model_VGG19 = keras.applications.VGG19(\n",
    "    weights=\"imagenet\",  # Load weights pre-trained on ImageNet.\n",
    "    input_shape=(IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_DEPTH),\n",
    "    include_top=False,\n",
    "    classifier_activation=\"softmax\"\n",
    ")  # Do not include the ImageNet classifier at the top.\n",
    "\n",
    "# Freeze the base_model\n",
    "base_model_VGG19.trainable = False\n",
    "\n",
    "data_augmentation = keras.Sequential(\n",
    "    [layers.RandomFlip(\"horizontal\"), layers.RandomRotation(0.1),]\n",
    ")\n",
    "\n",
    "\n",
    "# Create new model on top\n",
    "inputs = keras.Input(shape=(IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_DEPTH))\n",
    "x = data_augmentation(inputs)  # Apply random data augmentation\n",
    "\n",
    "\n",
    "scale_layer = keras.layers.Rescaling(scale=1 / 127.5, offset=-1)\n",
    "x = scale_layer(x)\n",
    "\n",
    "# The base model contains batchnorm layers. We want to keep them in inference mode\n",
    "# when we unfreeze the base model for fine-tuning, so we make sure that the\n",
    "# base_model is running in inference mode here.\n",
    "x = base_model_VGG19(x, training=False)\n",
    "x = keras.layers.GlobalAveragePooling2D()(x)\n",
    "x = keras.layers.Dropout(0.2)(x)  # Regularize with dropout\n",
    "outputs = keras.layers.Dense(nb_classes, activation='softmax')(x)\n",
    "model_VGG19 = keras.Model(inputs, outputs)\n",
    "\n",
    "model_VGG19.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model_VGG19.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "# # Add checkpoint function to save best model\n",
    "checkpoint = ModelCheckpoint(f'{WEIGHTS_DIR}/best_model_vgg19.hdf5', \n",
    "                             monitor = 'val_accuracy', \n",
    "                             verbose = 0,\n",
    "                             save_best_only=True,\n",
    "                             save_weights_only=True,\n",
    "                             mode = 'max')\n",
    "\n",
    "# Add LR scheduler \n",
    "lr_scheduler = ReduceLROnPlateau(monitor='val_loss',\n",
    "                              factor=0.2, #let's reduce LR 5 times\n",
    "                              patience=3, # if no improvement after 3 epoch - reduce LR\n",
    "                              min_lr=0.0001,\n",
    "                              verbose=1,\n",
    "                              mode='auto')\n",
    "\n",
    "# Add early stop\n",
    "earlystop = EarlyStopping(monitor = 'val_accuracy',\n",
    "                          patience = 10,\n",
    "                          restore_best_weights = True)\n",
    "\n",
    "callbacks_list = [earlystop, lr_scheduler, checkpoint]\n",
    "\n",
    "epochs = 40\n",
    "history = model_VGG19.fit(X_train, y_train, batch_size=12, epochs=epochs, \n",
    "                    validation_data=(X_test, y_test),\n",
    "                    callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard\n",
    "\n",
    "!del -rf ./logs_v\n",
    "log_dir = \"logs_v/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unfreeze the base_model. Note that it keeps running in inference mode\n",
    "# since we passed `training=False` when calling it. This means that\n",
    "# the batchnorm layers will not update their batch statistics.\n",
    "# This prevents the batchnorm layers from undoing all the training\n",
    "# we've done so far.\n",
    "%%time\n",
    "base_model_VGG19.trainable = True\n",
    "model_VGG19.summary()\n",
    "\n",
    "model_VGG19.compile(\n",
    "    optimizer=keras.optimizers.Adam(1e-5),  # Low learning rate\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "\n",
    "# # Add checkpoint function to save best model\n",
    "checkpoint = ModelCheckpoint(f'{WEIGHTS_DIR}/best_model_vgg19.hdf5', \n",
    "                             monitor = 'val_accuracy', \n",
    "                             verbose = 0,\n",
    "                             save_best_only=True,\n",
    "                             save_weights_only=True,\n",
    "                             mode = 'max')\n",
    "\n",
    "# Add LR scheduler \n",
    "lr_scheduler = ReduceLROnPlateau(monitor='val_loss',\n",
    "                              factor=0.1, \n",
    "                              patience=3, # if no improvement after 3 epoch - reduce LR\n",
    "                              min_lr=0.000001, \n",
    "                              verbose=1,\n",
    "                              mode='auto')\n",
    "\n",
    "# Add early stop\n",
    "earlystop = EarlyStopping(monitor = 'val_accuracy',\n",
    "                          patience = 10,\n",
    "                          restore_best_weights = True)\n",
    "\n",
    "callbacks_list = [earlystop, lr_scheduler, tensorboard_callback, checkpoint]\n",
    "\n",
    "\n",
    "\n",
    "epochs = 40\n",
    "history = model_VGG19.fit(X_train, y_train, batch_size=24, epochs=epochs, \n",
    "                    validation_data=(X_test, y_test),\n",
    "                    callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs_v/fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vgg19.load_weights(f'{WEIGHTS_DIR}/best_model_vgg19.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(model_vgg19, 3, MODELS_DIR, TARGET_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models evalutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_evaluation = {\"Model\": [\"CNN neural network\", \"SVM\", \"Xception\", \"VGG19\"], \n",
    "                   \"Input shape\": f\"{IMAGE_WIDTH}x{IMAGE_HEIGHT}x{IMAGE_DEPTH}\",\n",
    "                   \"Accuracy\":\"\",\n",
    "                   \"Precision\":\"\",\n",
    "                   \"Recall\":\"\"\n",
    "                   }\n",
    "evaluation_df = pd.DataFrame(dict_evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_metrics(test_df, TARGET_NAME, y_pred, y_test, X_test, model, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_df['Accuracy'][0] = evaluation_metrics(test_df, TARGET_NAME, y_pred, y_test, X_test, model, 0, 0)[0]\n",
    "evaluation_df['Precision'][0] = evaluation_metrics(test_df, TARGET_NAME, y_pred, y_test, X_test, model, 0, 0)[1]\n",
    "evaluation_df['Recall'][0] = evaluation_metrics(test_df, TARGET_NAME, y_pred, y_test, X_test, model, 0, 0)[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_svm = model_svm.predict(X_test_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_svm[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_metrics(test_df, TARGET_NAME, y_pred_svm, y_test_svm, X_test_svm, model_svm, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_df['Accuracy'][1] = evaluation_metrics(test_df, 'manufacturer', y_pred_svm, y_test_svm, X_test_svm, model_svm, 1, 0)[0]\n",
    "evaluation_df['Precision'][1] = evaluation_metrics(test_df, 'manufacturer', y_pred_svm, y_test_svm, X_test_svm, model_svm, 1, 0)[1]\n",
    "evaluation_df['Recall'][1] = evaluation_metrics(test_df, 'manufacturer', y_pred_svm, y_test_svm, X_test_svm, model_svm, 1, 0)[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred_svm_grid = model_svm_grid.predict(X_test_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred_svm_grid[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation_metrics(test_df, TARGET_NAME, y_pred_svm_grid, y_test_svm, X_test_svm, model_svm_grid, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation_df['Accuracy'][1] = accuracy_score(y_pred_svm_grid,y_test)\n",
    "# evaluation_df['Precision'][1] = accuracy_score(y_pred_svm_grid,y_test)\n",
    "# evaluation_df['Recall'][1] = accuracy_score(y_pred_svm_grid,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Xception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_Xception = model_Xception.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_Xception[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_metrics(test_df, TARGET_NAME, y_pred_Xception, y_test, X_test, model_Xception, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_df['Accuracy'][2] = evaluation_metrics(test_df, TARGET_NAME, y_pred_Xception, y_test, X_test, model_Xception, 0, 0)[0]\n",
    "evaluation_df['Precision'][2] = evaluation_metrics(test_df, TARGET_NAME, y_pred_Xception, y_test, X_test, model_Xception, 0, 0)[1]\n",
    "evaluation_df['Recall'][2] = evaluation_metrics(test_df, TARGET_NAME, y_pred_Xception, y_test, X_test, model_Xception, 0, 0)[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_VGG19 = model_VGG19.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_VGG19[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_metrics(test_df, TARGET_NAME, y_pred_VGG19, y_test, X_test, model_VGG19, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_df['Accuracy'][3] = evaluation_metrics(test_df, TARGET_NAME, y_pred_VGG19, y_test, X_test, model_VGG19, 0, 0)[0]\n",
    "evaluation_df['Precision'][3] = evaluation_metrics(test_df, TARGET_NAME, y_pred_VGG19, y_test, X_test, model_VGG19, 0, 0)[1]\n",
    "evaluation_df['Recall'][3] = evaluation_metrics(test_df, TARGET_NAME, y_pred_VGG19, y_test, X_test, model_VGG19, 0, 0)[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_df[evaluation_df[\"Accuracy\"] == evaluation_df[\"Accuracy\"].max()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_df.to_csv(f'{MODELS_DIR}/evaluation_df2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Génération du fichier requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip freeze | findstr \"imread= keras= matplotlib= numpy= pandas= pickle= pil= pyyaml= PyYAML= scikit-learn= scikit-image= seaborn= streamlit= tensorflow= yaml=\" > ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip freeze | findstr \"==\" > ../requirements.txt"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a6f10ab51804c8d7f0a9cd32ddf5981f4c2be07d4c4cc33857071c7ed4b9afa7"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
